# data
data:
  instruct_data: "/mnt/lawbotica-disk-1/datasets/complaints_summarize/train.jsonl"  # Fill
  data: ""  # Optionally fill with pretraining data 
  eval_instruct_data: "/mnt/lawbotica-disk-1/datasets/complaints_summarize/validation.jsonl"  # Optionally fill

# model
model_id_or_path: "/mnt/lawbotica-disk-1/models/Mistral-7B-Instruct-v0.3/"
lora:
  rank: 64

# optim
seq_len: 8192
batch_size: 1
max_steps: 50
optim:
  lr: 6.e-5
  weight_decay: 0.1
  pct_start: 0.05

# other
seed: 0
log_freq: 1
eval_freq: 50
no_eval: False
ckpt_freq: 50

save_adapters: True  # save only trained LoRA adapters. Set to `False` to merge LoRA adapter into the base model and save full fine-tuned model

run_dir: "/mnt/lawbotica-disk-1/runs/test-2"  # Fill

# wandb:
#   project: "" # your wandb project name
#   run_name: "" # your wandb run name
#   key: "" # your wandb api key
#   offline: False